{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import ltn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data = pd.read_csv('src\\data\\Stud_E-mobility_data_staticLimit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data = s_data[['_time','GARAGE_EXTERNAL_POWER', 'DEMAND_LIMIT',\n",
    "      #  'DEMAND_LIMIT_INDICATOR', \n",
    "       'BATTERY_SOC', 'BATTERY_DISCHARGE_POWER',\n",
    "       'BATTERY_CHARGED_ENERGY', 'BATTERY_DISCHARGED_ENERGY', 'PV_POWER',\n",
    "       'PV_ENERGY'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_charging(row):\n",
    "    if row[\"BATTERY_SOC\"] > 80:\n",
    "        return \"Fully Covered by Local Battery\"\n",
    "    elif 40 <= row[\"BATTERY_SOC\"] < 80:\n",
    "        if row[\"GARAGE_EXTERNAL_POWER\"] > row[\"DEMAND_LIMIT\"]:\n",
    "            return \"Partially Covered by Local Battery\"\n",
    "        else:\n",
    "            return \"Battery Charged from Grid\"\n",
    "    elif 15 <= row[\"BATTERY_SOC\"] <= 40:\n",
    "        if row[\"GARAGE_EXTERNAL_POWER\"] > row[\"DEMAND_LIMIT\"]:\n",
    "            return \"Partially Covered by Local Battery\"\n",
    "        else:\n",
    "            return \"Battery Charged from Grid\"\n",
    "    elif row[\"BATTERY_SOC\"] < 15:\n",
    "        return \"Battery Discharge Stopped due to Battery Health\"\n",
    "    else:\n",
    "        print(row[\"BATTERY_SOC\"])\n",
    "        print(row[\"GARAGE_EXTERNAL_POWER\"])\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply the labeling function to create the new column \"DRAWN_FROM\"\n",
    "s_data[\"DRAWN_FROM\"] = s_data.apply(label_charging, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = s_data.drop(['_time','DRAWN_FROM', 'BATTERY_DISCHARGE_POWER', 'BATTERY_CHARGED_ENERGY',  'BATTERY_DISCHARGED_ENERGY', 'GARAGE_EXTERNAL_POWER'], axis=1)\n",
    "target = s_data['DRAWN_FROM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAWN_FROM\n",
      "Battery Charged from Grid                          54783\n",
      "Partially Covered by Local Battery                  4457\n",
      "Battery Discharge Stopped due to Battery Health      202\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# calculate number of points in each class\n",
    "print(target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor implementation tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Dataset:\n",
    "\n",
    "#   def __init__(self, samples, labels, batch_size = 32):\n",
    "\n",
    "#     self.samples = samples\n",
    "#     self.labels = labels\n",
    "\n",
    "#     self.batch_size = batch_size\n",
    "\n",
    "#     self.length = int(np.ceil(samples.shape[0]/batch_size))\n",
    "\n",
    "#     self.indices = np.arange(samples.shape[0]) \n",
    "\n",
    "#   def __getitem__(self, i):\n",
    "\n",
    "#     i0 = i*self.batch_size\n",
    "#     i1 = min((i + 1)*self.batch_size, self.samples.shape[0])\n",
    "\n",
    "#     index = self.indices[i0:i1]\n",
    "\n",
    "#     return self.samples[index], self.labels[index]\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return self.length\n",
    "\n",
    "#   def shuffle(self):\n",
    "#     self.indices = np.random.permutation(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class SubNetworkTF(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "#         super().__init__()\n",
    "#         ks = (kernel_size, kernel_size)\n",
    "#         self.f = nn.Sequential(\n",
    "#             # Adjust the number of input and output channels\n",
    "#             nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=ks, stride=1, padding=0),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.f(x)\n",
    "\n",
    "# class NetworkTF(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.f = nn.Sequential(\n",
    "#             # The first block takes 1 input channel and produces 16 output channels\n",
    "#             SubNetworkTF(in_channels=1, out_channels=16),\n",
    "#             # The second block takes 16 input channels and produces 64 output channels\n",
    "#             SubNetworkTF(in_channels=16, out_channels=64),\n",
    "\n",
    "#             # Add a convolution layer with kernel size of 4 and 10 output channels\n",
    "#             nn.Conv2d(in_channels=64, out_channels=10, kernel_size=(4, 4), stride=1, padding=0),\n",
    "            \n",
    "#             # Flatten the output of the last convolution layer\n",
    "#             nn.Flatten(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.f(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def fit(model, number_of_epochs, train_data, train_labels, val_data, val_labels):\n",
    "#     # Define the CrossEntropyLoss and SGD optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.1)  \n",
    "\n",
    "#     # Lists to store training and validation losses\n",
    "#     training_losses = []\n",
    "#     validation_losses = []\n",
    "\n",
    "#     best_model = None\n",
    "#     best_val_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "#     for epoch in range(number_of_epochs):\n",
    "#         # Set the model to training mode\n",
    "#         model.train()\n",
    "\n",
    "#         # Forward pass\n",
    "#         train_outputs = model(train_data)\n",
    "#         train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         train_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Set the model to evaluation mode\n",
    "#         model.eval()\n",
    "\n",
    "#         # Forward pass for validation\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(val_data)\n",
    "#             val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "#         # Save training and validation losses\n",
    "#         training_losses.append(train_loss.item())\n",
    "#         validation_losses.append(val_loss.item())\n",
    "\n",
    "#         # Update best model if current validation loss is lower\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             best_model = model\n",
    "\n",
    "#         print(f'Epoch [{epoch + 1}/{number_of_epochs}], '\n",
    "#               f'Training Loss: {train_loss.item():.4f}, '\n",
    "#               f'Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "#     return best_model, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to convert labels to one-hot encoding\n",
    "# def one_hot_encode(labels, num_classes):\n",
    "#     return F.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "# def preprocess_data(samples, labels):\n",
    "#     print(labels)\n",
    "#     labels = torch.Tensor(labels)  # Convert labels to PyTorch Tensor\n",
    "#     labels_one_hot = one_hot_encode(labels.long(), num_classes=3)  # Assuming 3 classes\n",
    "#     return torch.Tensor(samples.values), labels_one_hot  # Convert DataFrame to numpy array before converting to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the data into training and validation sets\n",
    "# train_samples, val_samples, train_labels, val_labels = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(val_labels)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Now you can preprocess the data\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_samples, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m(train_samples, train_labels)\n\u001b[0;32m     12\u001b[0m val_samples, val_labels \u001b[38;5;241m=\u001b[39m preprocess_data(val_samples, val_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_data' is not defined"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Instantiate the encoder\n",
    "# encoder = LabelEncoder()\n",
    "\n",
    "# # Fit and transform the labels\n",
    "# train_labels = encoder.fit_transform(train_labels)\n",
    "# val_labels = encoder.transform(val_labels)\n",
    "\n",
    "# # Now you can preprocess the data\n",
    "# train_samples, train_labels = preprocess_data(train_samples, train_labels)\n",
    "# val_samples, val_labels = preprocess_data(val_samples, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = TensorDataset(train_samples, train_labels)\n",
    "# val_dataset = TensorDataset(val_samples, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_model = NetworkTF()\n",
    "# x,y = train_dataset[0]\n",
    "# vx,vy = val_dataset[0]\n",
    "# # y = y.argmax(dim=1)\n",
    "# # vy = vy.argmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "# best_pytorch_model, pytorch_train_losses, pytorch_val_losses = fit(tf_model, num_epochs, x, y, vx, vy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic Tensor Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "en_targ = encoder.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique values of the target\n",
    "unique_values = np.unique(en_targ)\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, en_targ, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = torch.tensor(features_train.to_numpy()).float()\n",
    "# target_train = torch.tensor(target_train.to_numpy()).long()\n",
    "features_test = torch.tensor(features_test.to_numpy()).float()\n",
    "# target_test = torch.tensor(target_test.to_numpy()).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_A = ltn.Constant(torch.tensor([1, 0, 0]))\n",
    "l_B = ltn.Constant(torch.tensor([0, 1, 0]))\n",
    "l_C = ltn.Constant(torch.tensor([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes=(4, 100, 52, 52, 3)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
    "                                                  for i in range(1, len(layer_sizes))])\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "            if training:\n",
    "                x = self.dropout(x)\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        return logits\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label l. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class l.\n",
    "    \"\"\"\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, l, training=False):\n",
    "        logits = self.logits_model(x, training=training)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "P = ltn.Predicate(LogitsToPredicate(mlp))\n",
    "\n",
    "# we define the connectives, quantifiers, and the SatAgg\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.unique_labels = np.unique(labels) \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Create a list of indices for each class\n",
    "        indices_per_class = {label: np.where(self.labels == label)[0] for label in self.unique_labels}\n",
    "\n",
    "        # Calculate the number of samples per class in each batch\n",
    "        samples_per_class = self.batch_size // len(self.unique_labels)\n",
    "\n",
    "        for _ in range(len(self)):\n",
    "            batch_indices = []\n",
    "\n",
    "            for label in self.unique_labels:\n",
    "                # Randomly sample indices for this class\n",
    "                class_indices = np.random.choice(indices_per_class[label], size=samples_per_class, replace=True)\n",
    "                batch_indices.extend(class_indices)\n",
    "\n",
    "            # If the batch size is not a multiple of the number of classes, fill the rest of the batch randomly\n",
    "            if len(batch_indices) < self.batch_size:\n",
    "                extra_indices = np.random.choice(np.arange(len(self.labels)), size=self.batch_size - len(batch_indices))\n",
    "                batch_indices.extend(extra_indices)\n",
    "\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(batch_indices)\n",
    "\n",
    "            yield self.data[batch_indices], self.labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels in loader:\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels == 0])\n",
    "        x_B = ltn.Variable(\"x_B\", data[labels == 1])\n",
    "        x_C = ltn.Variable(\"x_C\", data[labels == 2])\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_A, P(x_A, l_A)),\n",
    "            Forall(x_B, P(x_B, l_B)),\n",
    "            Forall(x_C, P(x_C, l_C))\n",
    "        )\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(loader):\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels in loader:\n",
    "        predictions = mlp(data).detach().numpy()\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        mean_accuracy += accuracy_score(labels, predictions)\n",
    "\n",
    "    return mean_accuracy / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# features_train, features_test, target_train, target_test\n",
    "# create train and test loader\n",
    "train_loader = DataLoader(features_train, target_train, 256, shuffle=True)\n",
    "test_loader = DataLoader(features_test, target_test, 256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([43811,   165,  3577], dtype=int64))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique values of the target and how much there is of each\n",
    "unique_values, counts = np.unique(target_train, return_counts=True)\n",
    "unique_values, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.2576 | Train Sat 0.746 | Test Sat 0.756 | Train Acc 0.919 | Test Acc 0.919\n",
      " epoch 20 | loss 0.2600 | Train Sat 0.748 | Test Sat 0.756 | Train Acc 0.921 | Test Acc 0.922\n",
      " epoch 40 | loss 0.2543 | Train Sat 0.747 | Test Sat 0.752 | Train Acc 0.923 | Test Acc 0.924\n",
      " epoch 60 | loss 0.2553 | Train Sat 0.747 | Test Sat 0.752 | Train Acc 0.926 | Test Acc 0.925\n",
      " epoch 80 | loss 0.2677 | Train Sat 0.752 | Test Sat 0.754 | Train Acc 0.921 | Test Acc 0.926\n",
      " epoch 100 | loss 0.2533 | Train Sat 0.746 | Test Sat 0.748 | Train Acc 0.924 | Test Acc 0.929\n",
      " epoch 120 | loss 0.2548 | Train Sat 0.743 | Test Sat 0.750 | Train Acc 0.920 | Test Acc 0.925\n",
      " epoch 140 | loss 0.2561 | Train Sat 0.751 | Test Sat 0.757 | Train Acc 0.924 | Test Acc 0.927\n",
      " epoch 160 | loss 0.2622 | Train Sat 0.750 | Test Sat 0.745 | Train Acc 0.922 | Test Acc 0.922\n",
      " epoch 180 | loss 0.2632 | Train Sat 0.749 | Test Sat 0.753 | Train Acc 0.927 | Test Acc 0.928\n",
      " epoch 200 | loss 0.2535 | Train Sat 0.746 | Test Sat 0.759 | Train Acc 0.925 | Test Acc 0.924\n",
      " epoch 220 | loss 0.2539 | Train Sat 0.748 | Test Sat 0.755 | Train Acc 0.924 | Test Acc 0.928\n",
      " epoch 240 | loss 0.2584 | Train Sat 0.746 | Test Sat 0.759 | Train Acc 0.921 | Test Acc 0.923\n",
      " epoch 260 | loss 0.2577 | Train Sat 0.743 | Test Sat 0.753 | Train Acc 0.922 | Test Acc 0.926\n",
      " epoch 280 | loss 0.2557 | Train Sat 0.746 | Test Sat 0.758 | Train Acc 0.926 | Test Acc 0.930\n",
      " epoch 300 | loss 0.2511 | Train Sat 0.752 | Test Sat 0.749 | Train Acc 0.927 | Test Acc 0.928\n",
      " epoch 320 | loss 0.2526 | Train Sat 0.755 | Test Sat 0.760 | Train Acc 0.926 | Test Acc 0.927\n",
      " epoch 340 | loss 0.2497 | Train Sat 0.752 | Test Sat 0.759 | Train Acc 0.927 | Test Acc 0.925\n",
      " epoch 360 | loss 0.2530 | Train Sat 0.750 | Test Sat 0.750 | Train Acc 0.923 | Test Acc 0.929\n",
      " epoch 380 | loss 0.2572 | Train Sat 0.750 | Test Sat 0.753 | Train Acc 0.924 | Test Acc 0.923\n",
      " epoch 400 | loss 0.2512 | Train Sat 0.752 | Test Sat 0.753 | Train Acc 0.927 | Test Acc 0.922\n",
      " epoch 420 | loss 0.2535 | Train Sat 0.750 | Test Sat 0.754 | Train Acc 0.927 | Test Acc 0.929\n",
      " epoch 440 | loss 0.2532 | Train Sat 0.753 | Test Sat 0.756 | Train Acc 0.923 | Test Acc 0.923\n",
      " epoch 460 | loss 0.2514 | Train Sat 0.750 | Test Sat 0.756 | Train Acc 0.927 | Test Acc 0.928\n",
      " epoch 480 | loss 0.2581 | Train Sat 0.755 | Test Sat 0.757 | Train Acc 0.928 | Test Acc 0.930\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(P.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # we ground the variables with current batch data\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels == 0]) # class A examples\n",
    "        x_B = ltn.Variable(\"x_B\", data[labels == 1]) # class B examples\n",
    "        x_C = ltn.Variable(\"x_C\", data[labels == 2]) # class C examples\n",
    "        # print(x_B)\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_A, P(x_A, l_A, training=True)),\n",
    "            Forall(x_B, P(x_B, l_B, training=True)),\n",
    "            Forall(x_C, P(x_C, l_C, training=True))\n",
    "        )\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"\n",
    "              %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
    "                    compute_accuracy(train_loader), compute_accuracy(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a matrix to store the counts\n",
    "class_counts = np.zeros((500, 3))\n",
    "\n",
    "for epoch in range(500):\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        # Count the number of examples in each class\n",
    "        class_counts[epoch, 0] = np.sum(labels == 0)\n",
    "        class_counts[epoch, 1] = np.sum(labels == 1)\n",
    "        class_counts[epoch, 2] = np.sum(labels == 2)\n",
    "    \n",
    "# Convert the counts to a DataFrame\n",
    "df_class_counts = pd.DataFrame(class_counts, columns=['Class A', 'Class B', 'Class C'])\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(df_class_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we tried to again predict the peak shaving, and check for different predicates what is their satisfaction level.\n",
    "- it would make sense that if we have a predicate: SOC <= 20 stop peak shaving, than if the treshold is good (or the rule in general), than the satisfaction of this rule will be high.\n",
    "- This could also allow us to check the relation to the solar panels ? -> if the predicat that includes it does better than that that does not include it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_peak_shaving(row):\n",
    "    if row['GARAGE_EXTERNAL_POWER'] >= row['DEMAND_LIMIT']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "s_data['Peak_Shaving'] = s_data.apply(check_peak_shaving, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "battery_discharge_power_threshold = 0\n",
    "pv_power_threshold = 0.5\n",
    "\n",
    "def adjust_peak_shaving(row):\n",
    "    if row['Peak_Shaving'] == True:\n",
    "        if row['BATTERY_DISCHARGE_POWER'] > battery_discharge_power_threshold or row['PV_POWER'] > pv_power_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data['Peak_Shaving'] = s_data.apply(adjust_peak_shaving, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_time</th>\n",
       "      <th>GARAGE_EXTERNAL_POWER</th>\n",
       "      <th>DEMAND_LIMIT</th>\n",
       "      <th>BATTERY_SOC</th>\n",
       "      <th>BATTERY_DISCHARGE_POWER</th>\n",
       "      <th>BATTERY_CHARGED_ENERGY</th>\n",
       "      <th>BATTERY_DISCHARGED_ENERGY</th>\n",
       "      <th>PV_POWER</th>\n",
       "      <th>PV_ENERGY</th>\n",
       "      <th>Peak_Shaving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-05 00:01:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-05 00:02:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-05 00:03:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-05 00:04:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-05 00:05:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.5</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59437</th>\n",
       "      <td>2023-11-18 23:55:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59438</th>\n",
       "      <td>2023-11-18 23:56:00+00:00</td>\n",
       "      <td>1.148468</td>\n",
       "      <td>50.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59439</th>\n",
       "      <td>2023-11-18 23:57:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59440</th>\n",
       "      <td>2023-11-18 23:58:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59441</th>\n",
       "      <td>2023-11-18 23:59:00+00:00</td>\n",
       "      <td>1.244171</td>\n",
       "      <td>50.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _time  GARAGE_EXTERNAL_POWER  DEMAND_LIMIT  \\\n",
       "0      2023-10-05 00:01:00+00:00               1.244171          50.0   \n",
       "1      2023-10-05 00:02:00+00:00               1.244171          50.0   \n",
       "2      2023-10-05 00:03:00+00:00               1.244171          50.0   \n",
       "3      2023-10-05 00:04:00+00:00               1.244171          50.0   \n",
       "4      2023-10-05 00:05:00+00:00               1.244171          50.0   \n",
       "...                          ...                    ...           ...   \n",
       "59437  2023-11-18 23:55:00+00:00               1.244171          50.0   \n",
       "59438  2023-11-18 23:56:00+00:00               1.148468          50.0   \n",
       "59439  2023-11-18 23:57:00+00:00               1.244171          50.0   \n",
       "59440  2023-11-18 23:58:00+00:00               1.244171          50.0   \n",
       "59441  2023-11-18 23:59:00+00:00               1.244171          50.0   \n",
       "\n",
       "       BATTERY_SOC  BATTERY_DISCHARGE_POWER  BATTERY_CHARGED_ENERGY  \\\n",
       "0             40.5                   -0.338                     0.0   \n",
       "1             40.5                   -0.372                     0.0   \n",
       "2             40.5                   -0.393                     0.0   \n",
       "3             40.5                   -0.339                     0.0   \n",
       "4             40.5                   -0.371                     0.0   \n",
       "...            ...                      ...                     ...   \n",
       "59437         41.0                   -0.380                     0.0   \n",
       "59438         41.0                   -0.458                     0.0   \n",
       "59439         41.0                   -0.437                     0.0   \n",
       "59440         41.0                   -0.476                     0.0   \n",
       "59441         41.0                   -0.487                     0.0   \n",
       "\n",
       "       BATTERY_DISCHARGED_ENERGY  PV_POWER  PV_ENERGY  Peak_Shaving  \n",
       "0                            0.0  0.008693   0.000000         False  \n",
       "1                            0.0  0.008693   0.000000         False  \n",
       "2                            0.0  0.008693   0.000000         False  \n",
       "3                            0.0  0.008693   0.000000         False  \n",
       "4                            0.0  0.008693   0.000000         False  \n",
       "...                          ...       ...        ...           ...  \n",
       "59437                        0.0  0.007606   0.000000         False  \n",
       "59438                        0.0  0.007606   0.000000         False  \n",
       "59439                        0.0  0.007705   0.000000         False  \n",
       "59440                        0.0  0.007705   0.000000         False  \n",
       "59441                        0.0  0.007705   0.003906         False  \n",
       "\n",
       "[59442 rows x 10 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = s_data.drop(['_time','Peak_Shaving', 'DEMAND_LIMIT', 'BATTERY_DISCHARGE_POWER', 'BATTERY_CHARGED_ENERGY',  'BATTERY_DISCHARGED_ENERGY'], axis=1)\n",
    "target = s_data['Peak_Shaving']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GARAGE_EXTERNAL_POWER</th>\n",
       "      <th>BATTERY_SOC</th>\n",
       "      <th>PV_POWER</th>\n",
       "      <th>PV_ENERGY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59437</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59438</th>\n",
       "      <td>1.148468</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59439</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59440</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59441</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59442 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GARAGE_EXTERNAL_POWER  BATTERY_SOC  PV_POWER  PV_ENERGY\n",
       "0                   1.244171         40.5  0.008693   0.000000\n",
       "1                   1.244171         40.5  0.008693   0.000000\n",
       "2                   1.244171         40.5  0.008693   0.000000\n",
       "3                   1.244171         40.5  0.008693   0.000000\n",
       "4                   1.244171         40.5  0.008693   0.000000\n",
       "...                      ...          ...       ...        ...\n",
       "59437               1.244171         41.0  0.007606   0.000000\n",
       "59438               1.148468         41.0  0.007606   0.000000\n",
       "59439               1.244171         41.0  0.007705   0.000000\n",
       "59440               1.244171         41.0  0.007705   0.000000\n",
       "59441               1.244171         41.0  0.007705   0.003906\n",
       "\n",
       "[59442 rows x 4 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = torch.tensor(features_train.to_numpy()).float()\n",
    "target_train = torch.tensor(target_train.to_numpy()).float()\n",
    "features_test = torch.tensor(features_test.to_numpy()).float()\n",
    "target_test = torch.tensor(target_test.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try first with a simple rule \n",
    "if GARAGE_EXTERNAL_POWER > DEMAND_LIMIT and SOC >= 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltn\n",
    "# we define predicate A\n",
    "class ModelA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.layer1 = torch.nn.Linear(4, 16)  \n",
    "        self.layer2 = torch.nn.Linear(16, 16)\n",
    "        self.layer3 = torch.nn.Linear(16, 1)\n",
    "        self.elu = torch.nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.layer1(x))\n",
    "        x = self.elu(self.layer2(x))\n",
    "        return self.sigmoid(self.layer3(x))\n",
    "\n",
    "\n",
    "A = ltn.Predicate(ModelA())\n",
    "\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# this is a standard PyTorch DataLoader to load the dataset for the training and testing of the model\n",
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.unique_labels = np.unique(labels) \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Create a list of indices for each class\n",
    "        indices_per_class = {label: np.where(self.labels == label)[0] for label in self.unique_labels}\n",
    "\n",
    "        # Calculate the number of samples per class in each batch\n",
    "        samples_per_class = max(1, self.batch_size // len(self.unique_labels))\n",
    "\n",
    "        for _ in range(len(self)):\n",
    "            batch_indices = []\n",
    "\n",
    "            for label in self.unique_labels:\n",
    "                # Randomly sample indices for this class\n",
    "                class_indices = np.random.choice(indices_per_class[label], size=samples_per_class, replace=True)\n",
    "                batch_indices.extend(class_indices)\n",
    "\n",
    "            # If the batch size is not a multiple of the number of classes, fill the rest of the batch randomly\n",
    "            if len(batch_indices) < self.batch_size:\n",
    "                extra_indices = np.random.choice(np.arange(len(self.labels)), size=self.batch_size - len(batch_indices))\n",
    "                batch_indices.extend(extra_indices)\n",
    "\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(batch_indices)\n",
    "\n",
    "            yield self.data[batch_indices], self.labels[batch_indices]\n",
    "\n",
    "\n",
    "# define metrics for evaluation of the model\n",
    "\n",
    "# it computes the overall satisfaction level on the knowledge base using the given data loader (train or test)\n",
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels in loader:\n",
    "        x_A = ltn.Variable(\"x_A\", data[torch.nonzero(labels)])  # positive examples\n",
    "        x_not_A = ltn.Variable(\"x_not_A\",\n",
    "                               data[torch.nonzero(torch.logical_not(labels))])  # negative examples\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_A, A(x_A)),\n",
    "            Forall(x_not_A, Not(A(x_not_A)))\n",
    "        )\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "# it computes the overall accuracy of the predictions of the trained model using the given data loader\n",
    "# (train or test)\n",
    "def compute_accuracy(loader):\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels in loader:\n",
    "        predictions = A.model(data).detach().numpy()\n",
    "        predictions = np.where(predictions > 0.5, 1., 0.).flatten()\n",
    "        mean_accuracy += accuracy_score(labels, predictions)\n",
    "\n",
    "    return mean_accuracy / len(loader)\n",
    "\n",
    "# create train and test loader, 50 points each\n",
    "# batch size is 64, meaning there is only one batch for epoch\n",
    "train_loader = DataLoader(features_train, target_train, 256, True)\n",
    "test_loader = DataLoader(features_test, target_test, 256, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def P(tensor):\n",
    "# #     print(tensor)\n",
    "# #     external_power, demand_limit, soc = tensor\n",
    "# #     return (external_power > demand_limit) & (soc >= 15)\n",
    "# def P(tensor):\n",
    "#     # print(tensor)\n",
    "#     external_power, soc = tensor.t()  # transpose the tensor\n",
    "#     return external_power > 50\n",
    "#     # return (external_power < demand_limit)\n",
    "#     # return (external_power > demand_limit) & (soc >= 15)\n",
    "\n",
    "# # Wrap the predicate in an LTN Predicate\n",
    "# P = ltn.Predicate(None, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Q(tensor):\n",
    "#     return tensor >= 0.5  # returns True if p_shav is True\n",
    "\n",
    "# # Wrap the predicate in an LTN Predicate\n",
    "# Q = ltn.Predicate(None, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soc_tensor = torch.tensor(s_data['BATTERY_SOC'].values, dtype=torch.float32)\n",
    "# external_power_tensor = torch.tensor(s_data['GARAGE_EXTERNAL_POWER'].values, dtype=torch.float32)\n",
    "# p_shav = torch.tensor(s_data['Peak_Shaving'].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# def phi(external_power_tensor, soc_tensor, p_shav):\n",
    "#     # Stack the tensors along the last dimension to create a single tensor\n",
    "#     data = torch.stack([external_power_tensor, soc_tensor], dim=-1)\n",
    "\n",
    "#     # Create a variable that represents the data\n",
    "#     p = ltn.Variable(\"p\", data)\n",
    "#     q = ltn.Variable(\"q\", p_shav)\n",
    "\n",
    "#     # Return the satisfaction degree of the formula\n",
    "#     return Forall(p, Implies(P(p), Q(q))).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(A.parameters(), lr=0.001)\n",
    "\n",
    "# training of the predicate A using a loss containing the satisfaction level of the knowledge base\n",
    "# the objective it to maximize the satisfaction level of the knowledge base\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # we ground the variables with current batch data\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels==1]) # positive examples\n",
    "        x_not_A = ltn.Variable(\"x_not_A\", data[labels==0]) # negative examples\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_A, A(x_A)),\n",
    "            Forall(x_not_A, Not(A(x_not_A)))\n",
    "        )\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 20 epochs of training\n",
    "    if epoch % 1 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"\n",
    "              %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
    "                    compute_accuracy(train_loader), compute_accuracy(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GARAGE_EXTERNAL_POWER</th>\n",
       "      <th>BATTERY_SOC</th>\n",
       "      <th>PV_POWER</th>\n",
       "      <th>PV_ENERGY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59437</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59438</th>\n",
       "      <td>1.148468</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59439</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59440</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59441</th>\n",
       "      <td>1.244171</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59442 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GARAGE_EXTERNAL_POWER  BATTERY_SOC  PV_POWER  PV_ENERGY\n",
       "0                   1.244171         40.5  0.008693   0.000000\n",
       "1                   1.244171         40.5  0.008693   0.000000\n",
       "2                   1.244171         40.5  0.008693   0.000000\n",
       "3                   1.244171         40.5  0.008693   0.000000\n",
       "4                   1.244171         40.5  0.008693   0.000000\n",
       "...                      ...          ...       ...        ...\n",
       "59437               1.244171         41.0  0.007606   0.000000\n",
       "59438               1.148468         41.0  0.007606   0.000000\n",
       "59439               1.244171         41.0  0.007705   0.000000\n",
       "59440               1.244171         41.0  0.007705   0.000000\n",
       "59441               1.244171         41.0  0.007705   0.003906\n",
       "\n",
       "[59442 rows x 4 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.2672 | Train Sat 0.733 | Test Sat 0.735 | Train Acc 0.914 | Test Acc 0.915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 21\u001b[0m\n\u001b[0;32m     15\u001b[0m sat_agg \u001b[38;5;241m=\u001b[39m SatAgg(\n\u001b[0;32m     16\u001b[0m     Forall(x_A, A(x_A)),\n\u001b[0;32m     17\u001b[0m     Forall(x_not_A, Not(A(x_not_A))),\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# phi(external_power_tensor, soc_tensor, p_shav)  # include phi in the satisfaction aggregation\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m sat_agg\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\adamr\\anaconda3\\envs\\LTN\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adamr\\anaconda3\\envs\\LTN\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adamr\\anaconda3\\envs\\LTN\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(A.parameters(), lr=0.001)\n",
    "\n",
    "# training of the predicate A using a loss containing the satisfaction level of the knowledge base\n",
    "# the objective it to maximize the satisfaction level of the knowledge base\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # we ground the variables with current batch data\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels==1]) # positive examples\n",
    "        x_not_A = ltn.Variable(\"x_not_A\", data[labels==0]) # negative examples\n",
    "        # print(data)\n",
    "        external_power_tensor, soc_tensor, _,_ = data.t()\n",
    "        p_shav = labels\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_A, A(x_A)),\n",
    "            Forall(x_not_A, Not(A(x_not_A))),\n",
    "            # phi(external_power_tensor, soc_tensor, p_shav)  # include phi in the satisfaction aggregation\n",
    "        )\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"# | Phi Sat %.3f\"\n",
    "              %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
    "                    compute_accuracy(train_loader), compute_accuracy(test_loader) ))#, phi(external_power_tensor, soc_tensor, p_shav)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
