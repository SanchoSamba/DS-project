{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import ltn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data = pd.read_csv('src\\data\\Stud_E-mobility_data_staticLimit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data = s_data[['_time','GARAGE_EXTERNAL_POWER', 'DEMAND_LIMIT',\n",
    "      #  'DEMAND_LIMIT_INDICATOR', \n",
    "       'BATTERY_SOC', 'BATTERY_DISCHARGE_POWER',\n",
    "       'BATTERY_CHARGED_ENERGY', 'BATTERY_DISCHARGED_ENERGY', 'PV_POWER',\n",
    "       'PV_ENERGY'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_charging(row):\n",
    "    if row[\"BATTERY_SOC\"] > 80:\n",
    "        return \"Fully Covered by Local Battery\"\n",
    "    elif 40 <= row[\"BATTERY_SOC\"] < 80:\n",
    "        if row[\"GARAGE_EXTERNAL_POWER\"] > row[\"DEMAND_LIMIT\"]:\n",
    "            return \"Partially Covered by Local Battery\"\n",
    "        else:\n",
    "            return \"Battery Charged from Grid\"\n",
    "    elif 15 <= row[\"BATTERY_SOC\"] <= 40:\n",
    "        if row[\"GARAGE_EXTERNAL_POWER\"] > row[\"DEMAND_LIMIT\"]:\n",
    "            return \"Partially Covered by Local Battery\"\n",
    "        else:\n",
    "            return \"Battery Charged from Grid\"\n",
    "    elif row[\"BATTERY_SOC\"] < 15:\n",
    "        return \"Battery Discharge Stopped due to Battery Health\"\n",
    "    else:\n",
    "        print(row[\"BATTERY_SOC\"])\n",
    "        print(row[\"GARAGE_EXTERNAL_POWER\"])\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply the labeling function to create the new column \"DRAWN_FROM\"\n",
    "s_data[\"DRAWN_FROM\"] = s_data.apply(label_charging, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = s_data.drop(['_time','DRAWN_FROM', 'BATTERY_DISCHARGE_POWER', 'BATTERY_CHARGED_ENERGY',  'BATTERY_DISCHARGED_ENERGY', 'GARAGE_EXTERNAL_POWER'], axis=1)\n",
    "target = s_data['DRAWN_FROM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAWN_FROM\n",
      "Battery Charged from Grid                          54783\n",
      "Partially Covered by Local Battery                  4457\n",
      "Battery Discharge Stopped due to Battery Health      202\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# calculate number of points in each class\n",
    "print(target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dataset:\n",
    "\n",
    "  def __init__(self, samples, labels, batch_size = 32):\n",
    "\n",
    "    self.samples = samples\n",
    "    self.labels = labels\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    self.length = int(np.ceil(samples.shape[0]/batch_size))\n",
    "\n",
    "    self.indices = np.arange(samples.shape[0]) \n",
    "\n",
    "  def __getitem__(self, i):\n",
    "\n",
    "    i0 = i*self.batch_size\n",
    "    i1 = min((i + 1)*self.batch_size, self.samples.shape[0])\n",
    "\n",
    "    index = self.indices[i0:i1]\n",
    "\n",
    "    return self.samples[index], self.labels[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "  def shuffle(self):\n",
    "    self.indices = np.random.permutation(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SubNetworkTF(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        ks = (kernel_size, kernel_size)\n",
    "        self.f = nn.Sequential(\n",
    "            # Adjust the number of input and output channels\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=ks, stride=1, padding=0),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "class NetworkTF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.f = nn.Sequential(\n",
    "            # The first block takes 1 input channel and produces 16 output channels\n",
    "            SubNetworkTF(in_channels=1, out_channels=16),\n",
    "            # The second block takes 16 input channels and produces 64 output channels\n",
    "            SubNetworkTF(in_channels=16, out_channels=64),\n",
    "\n",
    "            # Add a convolution layer with kernel size of 4 and 10 output channels\n",
    "            nn.Conv2d(in_channels=64, out_channels=10, kernel_size=(4, 4), stride=1, padding=0),\n",
    "            \n",
    "            # Flatten the output of the last convolution layer\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(model, number_of_epochs, train_data, train_labels, val_data, val_labels):\n",
    "    # Define the CrossEntropyLoss and SGD optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)  \n",
    "\n",
    "    # Lists to store training and validation losses\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass\n",
    "        train_outputs = model(train_data)\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Forward pass for validation\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(val_data)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "        # Save training and validation losses\n",
    "        training_losses.append(train_loss.item())\n",
    "        validation_losses.append(val_loss.item())\n",
    "\n",
    "        # Update best model if current validation loss is lower\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{number_of_epochs}], '\n",
    "              f'Training Loss: {train_loss.item():.4f}, '\n",
    "              f'Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    return best_model, training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return F.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "def preprocess_data(samples, labels):\n",
    "    print(labels)\n",
    "    labels = torch.Tensor(labels)  # Convert labels to PyTorch Tensor\n",
    "    labels_one_hot = one_hot_encode(labels.long(), num_classes=3)  # Assuming 3 classes\n",
    "    return torch.Tensor(samples.values), labels_one_hot  # Convert DataFrame to numpy array before converting to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "train_samples, val_samples, train_labels, val_labels = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(val_labels)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Now you can preprocess the data\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_samples, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m(train_samples, train_labels)\n\u001b[0;32m     12\u001b[0m val_samples, val_labels \u001b[38;5;241m=\u001b[39m preprocess_data(val_samples, val_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate the encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "train_labels = encoder.fit_transform(train_labels)\n",
    "val_labels = encoder.transform(val_labels)\n",
    "\n",
    "# Now you can preprocess the data\n",
    "train_samples, train_labels = preprocess_data(train_samples, train_labels)\n",
    "val_samples, val_labels = preprocess_data(val_samples, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_samples, train_labels)\n",
    "val_dataset = TensorDataset(val_samples, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = NetworkTF()\n",
    "x,y = train_dataset[0]\n",
    "vx,vy = val_dataset[0]\n",
    "# y = y.argmax(dim=1)\n",
    "# vy = vy.argmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "# best_pytorch_model, pytorch_train_losses, pytorch_val_losses = fit(tf_model, num_epochs, x, y, vx, vy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic Tensor Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "en_targ = encoder.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique values of the target\n",
    "unique_values = np.unique(en_targ)\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, en_targ, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = torch.tensor(features_train.to_numpy()).float()\n",
    "# target_train = torch.tensor(target_train.to_numpy()).long()\n",
    "features_test = torch.tensor(features_test.to_numpy()).float()\n",
    "# target_test = torch.tensor(target_test.to_numpy()).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_A = ltn.Constant(torch.tensor([1, 0, 0]))\n",
    "l_B = ltn.Constant(torch.tensor([0, 1, 0]))\n",
    "l_C = ltn.Constant(torch.tensor([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes=(4, 100, 52, 52, 3)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
    "                                                  for i in range(1, len(layer_sizes))])\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "            if training:\n",
    "                x = self.dropout(x)\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        return logits\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label l. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class l.\n",
    "    \"\"\"\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, l, training=False):\n",
    "        logits = self.logits_model(x, training=training)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "P = ltn.Predicate(LogitsToPredicate(mlp))\n",
    "\n",
    "# we define the connectives, quantifiers, and the SatAgg\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.unique_labels = np.unique(labels) \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Create a list of indices for each class\n",
    "        indices_per_class = {label: np.where(self.labels == label)[0] for label in self.unique_labels}\n",
    "\n",
    "        # Calculate the number of samples per class in each batch\n",
    "        samples_per_class = self.batch_size // len(self.unique_labels)\n",
    "\n",
    "        for _ in range(len(self)):\n",
    "            batch_indices = []\n",
    "\n",
    "            for label in self.unique_labels:\n",
    "                # Randomly sample indices for this class\n",
    "                class_indices = np.random.choice(indices_per_class[label], size=samples_per_class, replace=True)\n",
    "                batch_indices.extend(class_indices)\n",
    "\n",
    "            # If the batch size is not a multiple of the number of classes, fill the rest of the batch randomly\n",
    "            if len(batch_indices) < self.batch_size:\n",
    "                extra_indices = np.random.choice(np.arange(len(self.labels)), size=self.batch_size - len(batch_indices))\n",
    "                batch_indices.extend(extra_indices)\n",
    "\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(batch_indices)\n",
    "\n",
    "            yield self.data[batch_indices], self.labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels in loader:\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels == 0])\n",
    "        x_B = ltn.Variable(\"x_B\", data[labels == 1])\n",
    "        x_C = ltn.Variable(\"x_C\", data[labels == 2])\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_A, P(x_A, l_A)),\n",
    "            Forall(x_B, P(x_B, l_B)),\n",
    "            Forall(x_C, P(x_C, l_C))\n",
    "        )\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(loader):\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels in loader:\n",
    "        predictions = mlp(data).detach().numpy()\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        mean_accuracy += accuracy_score(labels, predictions)\n",
    "\n",
    "    return mean_accuracy / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# features_train, features_test, target_train, target_test\n",
    "# create train and test loader\n",
    "train_loader = DataLoader(features_train, target_train, 256, shuffle=True)\n",
    "test_loader = DataLoader(features_test, target_test, 256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([43811,   165,  3577], dtype=int64))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique values of the target and how much there is of each\n",
    "unique_values, counts = np.unique(target_train, return_counts=True)\n",
    "unique_values, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.2576 | Train Sat 0.746 | Test Sat 0.756 | Train Acc 0.919 | Test Acc 0.919\n",
      " epoch 20 | loss 0.2600 | Train Sat 0.748 | Test Sat 0.756 | Train Acc 0.921 | Test Acc 0.922\n",
      " epoch 40 | loss 0.2543 | Train Sat 0.747 | Test Sat 0.752 | Train Acc 0.923 | Test Acc 0.924\n",
      " epoch 60 | loss 0.2553 | Train Sat 0.747 | Test Sat 0.752 | Train Acc 0.926 | Test Acc 0.925\n",
      " epoch 80 | loss 0.2677 | Train Sat 0.752 | Test Sat 0.754 | Train Acc 0.921 | Test Acc 0.926\n",
      " epoch 100 | loss 0.2533 | Train Sat 0.746 | Test Sat 0.748 | Train Acc 0.924 | Test Acc 0.929\n",
      " epoch 120 | loss 0.2548 | Train Sat 0.743 | Test Sat 0.750 | Train Acc 0.920 | Test Acc 0.925\n",
      " epoch 140 | loss 0.2561 | Train Sat 0.751 | Test Sat 0.757 | Train Acc 0.924 | Test Acc 0.927\n",
      " epoch 160 | loss 0.2622 | Train Sat 0.750 | Test Sat 0.745 | Train Acc 0.922 | Test Acc 0.922\n",
      " epoch 180 | loss 0.2632 | Train Sat 0.749 | Test Sat 0.753 | Train Acc 0.927 | Test Acc 0.928\n",
      " epoch 200 | loss 0.2535 | Train Sat 0.746 | Test Sat 0.759 | Train Acc 0.925 | Test Acc 0.924\n",
      " epoch 220 | loss 0.2539 | Train Sat 0.748 | Test Sat 0.755 | Train Acc 0.924 | Test Acc 0.928\n",
      " epoch 240 | loss 0.2584 | Train Sat 0.746 | Test Sat 0.759 | Train Acc 0.921 | Test Acc 0.923\n",
      " epoch 260 | loss 0.2577 | Train Sat 0.743 | Test Sat 0.753 | Train Acc 0.922 | Test Acc 0.926\n",
      " epoch 280 | loss 0.2557 | Train Sat 0.746 | Test Sat 0.758 | Train Acc 0.926 | Test Acc 0.930\n",
      " epoch 300 | loss 0.2511 | Train Sat 0.752 | Test Sat 0.749 | Train Acc 0.927 | Test Acc 0.928\n",
      " epoch 320 | loss 0.2526 | Train Sat 0.755 | Test Sat 0.760 | Train Acc 0.926 | Test Acc 0.927\n",
      " epoch 340 | loss 0.2497 | Train Sat 0.752 | Test Sat 0.759 | Train Acc 0.927 | Test Acc 0.925\n",
      " epoch 360 | loss 0.2530 | Train Sat 0.750 | Test Sat 0.750 | Train Acc 0.923 | Test Acc 0.929\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(P.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # we ground the variables with current batch data\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels == 0]) # class A examples\n",
    "        x_B = ltn.Variable(\"x_B\", data[labels == 1]) # class B examples\n",
    "        x_C = ltn.Variable(\"x_C\", data[labels == 2]) # class C examples\n",
    "        # print(x_B)\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_A, P(x_A, l_A, training=True)),\n",
    "            Forall(x_B, P(x_B, l_B, training=True)),\n",
    "            Forall(x_C, P(x_C, l_C, training=True))\n",
    "        )\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"\n",
    "              %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
    "                    compute_accuracy(train_loader), compute_accuracy(test_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
